{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7ef833-9139-4e78-ab83-341f3a033090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 256,\n",
    "    \"context_length\": 128,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8529aff7-16cc-424c-bb73-0adb88e64806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i+1:i+max_length+1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(text, batch_size=16, max_length=128, stride=64):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        x, y = zip(*batch)\n",
    "        return torch.stack(x), torch.stack(y)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return loader, tokenizer\n",
    "\n",
    "\n",
    "# Exemple de texte\n",
    "text = \"psychoanalysis is\" * 100\n",
    "dataloader, tokenizer = create_dataloader_v1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab3544e3-f180-4d18-9b92-fa9f3bcd2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim): super().__init__()\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        mask = self.mask[:T, :T].unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        attn = attn.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ln2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.att = MultiHeadAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"],\n",
    "                                      cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.att(self.ln1(x)))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embed = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.ln_f = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok = self.token_embed(x)\n",
    "        pos = self.pos_embed(torch.arange(T, device=x.device))\n",
    "        x = self.dropout(tok + pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9dc688-325e-4721-b18d-2bdd8e7c734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.13it/s, loss=165]\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(cfg).to(device)\n",
    "model.head.weight = model.token_embed.weight  # optional tying\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits.view(-1, cfg[\"vocab_size\"]), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95bcae3-caf4-4543-a79e-b098bbde4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psychoanalysis is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is is\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, input_ids, max_new_tokens=50, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)[:, -1, :] / temperature\n",
    "        if top_k:\n",
    "            top_vals, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < top_vals[:, [-1]]] = -float(\"inf\")\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    return input_ids\n",
    "\n",
    "# Test de génération\n",
    "prompt = \"psychoanalysis is\"\n",
    "input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)\n",
    "generated = generate(model, input_ids, max_new_tokens=50)\n",
    "print(tokenizer.decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb2c72-ba5c-4305-9400-ef5b86b0fde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
