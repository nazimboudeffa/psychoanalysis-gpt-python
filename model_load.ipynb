{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aea30a1-5aa8-4897-93bf-c8e95724108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is fast.\n",
      "they eats on the bed.\n",
      "the teacher jumps quietly.\n",
      "they sleeps milk.\n",
      "the teacher drinks walks with me.\n",
      "a bird sings at school.\n",
      "my friend reads fast\n"
     ]
    }
   ],
   "source": [
    "# Load and use PicoGPT (or custom GPT) in Jupyter Notebook\n",
    "\n",
    "# Step 1: Imports and Config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "\n",
    "# Define model config (adapt to your model)\n",
    "cfg = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 256,\n",
    "    \"context_length\": 128,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "# Step 2: Define GPTModel (must match training architecture)\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim): super().__init__()\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        mask = self.mask[:T, :T].unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        attn = attn.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ln2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.att = MultiHeadAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"],\n",
    "                                      cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.att(self.ln1(x)))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embed = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.ln_f = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok = self.token_embed(x)\n",
    "        pos = self.pos_embed(torch.arange(T, device=x.device))\n",
    "        x = self.dropout(tok + pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Step 3: Load pretrained model weights\n",
    "model = GPTModel(cfg)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"psychogpt.pt\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Step 4: Load tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Step 5: Define generation function\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, stop_token=\"<|endoftext|>\"):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -cfg[\"context_length\"]:]  # Tronque si la séquence devient trop longue\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Appliquer température\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        # Optionnel : arrêt sur token de fin\n",
    "        if tokenizer.decode([next_token.item()]) == stop_token:\n",
    "            break\n",
    "\n",
    "    decoded = tokenizer.decode(x[0].tolist())\n",
    "    return decoded\n",
    "\n",
    "# Step 6: Use the model\n",
    "prompt = \"psychoanalysis is\"\n",
    "print(generate(model, tokenizer, prompt, max_new_tokens=40, temperature=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579b594-ea67-474d-98e4-fc01d53eb89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
