{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4b1067-dbcd-49eb-b3c1-66bc382d3785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 256,\n",
    "    \"context_length\": 128,\n",
    "    \"n_heads\": 4,\n",
    "    \"n_layers\": 4,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffd8394-1393-4601-a0a3-f2dcbe866f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"a_general_introduction_to_psychoanalysis.txt\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text.strip().lower()\n",
    "text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d6b172-491f-43a7-ba75-1f56a43863c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        if not text.endswith(\"<|endoftext|>\"):\n",
    "            text += \" <|endoftext|>\"\n",
    "\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i+1:i+max_length+1]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(text, batch_size=16, max_length=128, stride=64):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        x, y = zip(*batch)\n",
    "        return torch.stack(x), torch.stack(y)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    return loader, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdca3f3-0ad1-4edc-8f6c-d45fded12e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim): super().__init__()\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / torch.sqrt(var + self.eps) + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        q = self.q(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        mask = self.mask[:T, :T].unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        attn = attn.masked_fill(mask.bool(), float(\"-inf\"))\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.ln2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.att = MultiHeadAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"context_length\"],\n",
    "                                      cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.att(self.ln1(x)))\n",
    "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_embed = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.ln_f = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        tok = self.token_embed(x)\n",
    "        pos = self.pos_embed(torch.arange(T, device=x.device))\n",
    "        x = self.dropout(tok + pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eadfc23a-92b9-4fe4-abf5-1687372daaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 6.4160: 100%|████████████████████████████████████████████████████████| 106/106 [03:59<00:00,  2.26s/it]\n",
      "Epoch 2 | Loss: 5.9216: 100%|████████████████████████████████████████████████████████| 106/106 [03:47<00:00,  2.15s/it]\n",
      "Epoch 3 | Loss: 5.5305: 100%|████████████████████████████████████████████████████████| 106/106 [03:51<00:00,  2.18s/it]\n",
      "Epoch 4 | Loss: 5.4178: 100%|████████████████████████████████████████████████████████| 106/106 [03:47<00:00,  2.14s/it]\n",
      "Epoch 5 | Loss: 5.0905: 100%|████████████████████████████████████████████████████████| 106/106 [03:40<00:00,  2.08s/it]\n",
      "Epoch 6 | Loss: 4.8296: 100%|████████████████████████████████████████████████████████| 106/106 [03:38<00:00,  2.06s/it]\n",
      "Epoch 7 | Loss: 4.3045: 100%|████████████████████████████████████████████████████████| 106/106 [03:37<00:00,  2.05s/it]\n",
      "Epoch 8 | Loss: 4.5775: 100%|████████████████████████████████████████████████████████| 106/106 [03:41<00:00,  2.09s/it]\n",
      "Epoch 9 | Loss: 4.2599: 100%|████████████████████████████████████████████████████████| 106/106 [03:43<00:00,  2.11s/it]\n",
      "Epoch 10 | Loss: 4.2582: 100%|███████████████████████████████████████████████████████| 106/106 [03:46<00:00,  2.13s/it]\n",
      "Epoch 11 | Loss: 4.2040: 100%|███████████████████████████████████████████████████████| 106/106 [03:45<00:00,  2.13s/it]\n",
      "Epoch 12 | Loss: 3.9990: 100%|███████████████████████████████████████████████████████| 106/106 [03:38<00:00,  2.06s/it]\n",
      "Epoch 13 | Loss: 3.8469: 100%|███████████████████████████████████████████████████████| 106/106 [03:48<00:00,  2.16s/it]\n",
      "Epoch 14 | Loss: 3.8279: 100%|███████████████████████████████████████████████████████| 106/106 [03:48<00:00,  2.15s/it]\n",
      "Epoch 15 | Loss: 3.5413: 100%|███████████████████████████████████████████████████████| 106/106 [03:45<00:00,  2.13s/it]\n",
      "Epoch 16 | Loss: 3.5287: 100%|███████████████████████████████████████████████████████| 106/106 [03:39<00:00,  2.08s/it]\n",
      "Epoch 17 | Loss: 3.4257: 100%|███████████████████████████████████████████████████████| 106/106 [03:37<00:00,  2.05s/it]\n",
      "Epoch 18 | Loss: 3.0904: 100%|███████████████████████████████████████████████████████| 106/106 [03:46<00:00,  2.14s/it]\n",
      "Epoch 19 | Loss: 3.1134: 100%|███████████████████████████████████████████████████████| 106/106 [03:51<00:00,  2.18s/it]\n",
      "Epoch 20 | Loss: 2.9314: 100%|███████████████████████████████████████████████████████| 106/106 [03:41<00:00,  2.09s/it]\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "loader, tokenizer = create_dataloader_v1(text, batch_size=32, max_length=cfg[\"context_length\"], stride=64)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f584008-e983-4365-8278-edc967a3b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=50, temperature=1.0, stop_token=\"<|endoftext|>\"):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -cfg[\"context_length\"]:]  # Tronque si la séquence devient trop longue\n",
    "        with torch.no_grad():\n",
    "            logits = model(x_cond)\n",
    "            logits = logits[:, -1, :] / temperature  # Appliquer température\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        x = torch.cat([x, next_token], dim=1)\n",
    "\n",
    "        # Optionnel : arrêt sur token de fin\n",
    "        if tokenizer.decode([next_token.item()]) == stop_token:\n",
    "            break\n",
    "\n",
    "    decoded = tokenizer.decode(x[0].tolist())\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6364abe3-a549-42b2-aa50-d1e57a3c537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generated text ===\n",
      "psychoanalysis is enough that it is\n",
      "was that it would be only in the dream. it is a dream should be a belief to\n",
      "the dream as it necessary to us. it is obvious far that in the dream becomes seems\n",
      "to the dream, and so\n"
     ]
    }
   ],
   "source": [
    "# Supposons que ton modèle est entraîné et que tu as :\n",
    "# - model (instance de GPTModel)\n",
    "# - tokenizer (tiktoken encoding GPT-2)\n",
    "\n",
    "prompt = \"psychoanalysis is\"\n",
    "generated_text = generate(model, tokenizer, prompt, max_new_tokens=50, temperature=0.8)\n",
    "\n",
    "print(\"=== Generated text ===\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4cd0bf-4cb4-4566-a723-1833dbc9f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"psycogpt.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
